{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#0084bb\">Regression in all Shapes and Sizes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the first assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with some of the simplest neural networks possible. \n",
    "Essentially, these simple networks come down to a well-known tool in statistics: **regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# data generation\n",
    "\n",
    "\n",
    "def plane_data(n=100, d=2, bound=3., noise=.1, offset=0., seed=None):\n",
    "    \"\"\"\n",
    "    Generate samples for a linear regression dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int, optional\n",
    "        Number of samples to generate.\n",
    "    d : int, optional\n",
    "        Number of dimensions in the input data.\n",
    "    bound : float, optional\n",
    "        Distance between center value and extreme values in the inputs.\n",
    "    noise : float, optional\n",
    "        Standard deviation for the additive Gaussian noise.\n",
    "    offset : float, optional\n",
    "        Amount by which to shift the output values.\n",
    "    seed : int, optional\n",
    "        Set this number to get reproducable results.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : (n, d) ndarray\n",
    "        Input data.\n",
    "    y : (n, 1) ndarray\n",
    "        Target values.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed or np.random.randint(256) + 1)\n",
    "    x = rng.uniform(-bound, bound, size=(n, d))\n",
    "    y = offset + x.mean(axis=-1) / bound\n",
    "    target = y + noise * rng.randn(n) + offset\n",
    "    return x, target.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def blob_data(n_per_blob=50, d=2, k=2, noise=.1, offset=0., seed=None):\n",
    "    \"\"\"\n",
    "    Generate samples for a classification dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_per_blob : int, optional\n",
    "        Number of samples per class to generate.\n",
    "    d : int, optional\n",
    "        Number of dimensions in the input data.\n",
    "    k : int, optional\n",
    "        Number of classes in the data.\n",
    "    noise : float, optional\n",
    "        Standard deviation for the additive Gaussian noise.\n",
    "    offset : float, optional\n",
    "        Amount by which to shift the input values.\n",
    "    seed : int, optional\n",
    "        Set this number to get reproducable results.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : (n, d) ndarray\n",
    "        Input data.\n",
    "    y : (n, ) ndarray\n",
    "        Target Labels.\n",
    "    \"\"\"\n",
    "    means = np.zeros(shape=(k, d)) + np.linspace(-1, 1, k)[:, None]\n",
    "    means += offset * d\n",
    "    stds = 10 * d / k * noise + np.zeros_like(means)\n",
    "    \n",
    "    rng = np.random.RandomState(seed or np.random.randint(256) + 1)\n",
    "    z = rng.randn(n_per_blob, k, d)\n",
    "    x = stds * z + means\n",
    "    y = np.ones(z.shape[:-1], dtype='int') * np.arange(k)\n",
    "    return x.reshape(-1, d), y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# visual functions\n",
    "\n",
    "\n",
    "def show_1d_model(x, y, w, b, pred_func):\n",
    "    \"\"\"Visualise 1D model on data\"\"\"\n",
    "    plt.scatter(x, y, color='steelblue')\n",
    "    _x = np.linspace(x.min(), x.max())[:, None]\n",
    "    _s = my_first_network(_x, w, b)\n",
    "    _pred = pred_func(_s)\n",
    "    plt.plot(_x, _pred, color='tomato', linewidth=3, label='prediction')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "def show_2d_model(x, y, w, b, pred_func, cmap='viridis'):\n",
    "    \"\"\"Visualise 2D model on data\"\"\"\n",
    "    _x = np.linspace(x.min(), x.max())\n",
    "    _x1, _x2 = np.meshgrid(_x, _x)\n",
    "    _x = np.c_[_x1.flat, _x2.flat]\n",
    "    _s = my_first_network(_x, w, b)\n",
    "    _pred = pred_func(_s)\n",
    "    _pred = _pred.reshape(_x1.shape)\n",
    "    vmin = min(_pred.min(), y.min())\n",
    "    vmax = max(_pred.max(), y.max())\n",
    "    plt.contour(_x1, _x2, _pred, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.scatter(*x.T, c=y.flatten(), cmap=cmap, vmin=vmin, vmax=vmax, edgecolors='w')\n",
    "    plt.colorbar().set_label('$y$')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.legend(handles=[plt.Line2D([], [], color='black', label='prediction contours')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0084bb\">Linear Regression</h2>\n",
    "\n",
    "Linear regression is a natural choice for modelling linear relationships in statistics. Ater all, it maximises the likelihood of a linear model if the noise in the data is assumed to be Gaussian. On top of that, the *Maximum Likelihood Estimator* can be written down analytically. Since this model corresponds to a single-layer neural network without activation function, it is a good starting point for exploring neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6 style=\"color:#0084bb\">Model</h6>\n",
    "\n",
    "Concretely, the outputs $\\boldsymbol{y} \\in \\mathbb{R}^K$ are assumed to depend on the inputs $\\boldsymbol{x} \\in \\mathbb{R}^D$ and random noise $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$ so that\n",
    "\n",
    "$$\\boldsymbol{y} = f(\\boldsymbol{x}) + \\boldsymbol{\\varepsilon},$$\n",
    "\n",
    "where $f : \\mathbb{R}^D \\to \\mathbb{R}^K$ is some linear function. \n",
    "To model the underlying relationship $f$, we will use one of the simplest possible *Neural Networks*:\n",
    "\n",
    "$$g(\\boldsymbol{x} \\mathbin{;} \\theta) = \\boldsymbol{w} \\cdot \\boldsymbol{x} + \\boldsymbol{b},$$\n",
    "\n",
    "where $\\theta = \\{\\boldsymbol{w}, \\boldsymbol{b}\\}$ is the set of parameters for the model.\n",
    "**Note** that this network corresponds to a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6 style=\"color:#0084bb\">Likelihood</h6>\n",
    "\n",
    "The *Likelihood* of a supervised machine learning model for a dataset with \n",
    "*identically and independenltly distributed* inputs $\\boldsymbol{X}$ and outputs $\\boldsymbol{Y}$ is given by\n",
    "\n",
    "$$\\mathcal{L}(\\theta \\mathbin{;} \\boldsymbol{X}, \\boldsymbol{Y}) = \\prod_{n = 1}^{N} p(\\boldsymbol{x}^n, \\boldsymbol{y}^n \\mathbin{;} \\theta) = \\prod_{n = 1}^{N} p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta).$$\n",
    "\n",
    "This likelihood quantises how likely some data is, given the parameters of a model.\n",
    "Given the model assumptions as stated above, the conditional probablity \n",
    "in the likelihood product can be written as \n",
    "\n",
    "$$p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) = p_\\mathcal{N}\\left(\\boldsymbol{y}^n \\mathbin{;} g(\\boldsymbol{x}^n \\mathbin{;} \\theta), \\sigma^2\\right) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(\\frac{-\\left(\\boldsymbol{y}^n - g(\\boldsymbol{x}^n \\mathbin{;} \\theta)\\right)^2}{2 \\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6 style=\"color:#0084bb\">Optimisation</h6>\n",
    "\n",
    "Maximising the likelihood, $\\mathcal{L}(\\theta)$, is equivalent to maximising the *log-likelihood*, $\\mathcal{l}(\\theta) = \\ln \\mathcal{L}(\\theta)$, or minimising the additive inverse. Therefore, the optimal parameters, $\\theta^*$ are specified by\n",
    "\n",
    "$$\\begin{align}\n",
    "  \\theta^* & = \\arg\\min_\\theta \\left\\{-\\mathcal{l}(\\theta \\mathbin{;} \\boldsymbol{X}, \\boldsymbol{Y}) \\right\\} \\\\\n",
    "  & = \\arg\\min_\\theta \\left\\{-\\sum_{n = 1}^{N} \\ln p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) \\right\\} \\\\\n",
    "  & = \\arg\\min_\\theta \\left\\{N \\ln \\sqrt{2 \\pi \\sigma^2} + \\frac{1}{2 \\sigma^2} \\sum_{n = 1}^{N}\\left(\\boldsymbol{y}^n - g(\\boldsymbol{x}^n \\mathbin{;} \\theta)\\right)^2 \\right\\},\n",
    "\\end{align}$$\n",
    "\n",
    "which is equivalent to minimising the sum of squared errors\n",
    "\n",
    "$$\\theta^* = \\arg\\min_\\theta \\left\\{\\frac{1}{2} \\sum_{n = 1}^{N}\\left(\\boldsymbol{y}^n - g(\\boldsymbol{x}^n \\mathbin{;} \\theta)\\right)^2 \\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 1: Numpy Refreshment (2 Points)</h3>\n",
    "\n",
    "The model described above should be straightforward to implement. How about you get familiar with [python](https://docs.python.org/3) and [numpy](https://docs.scipy.org/doc/numpy) (again)?\n",
    "\n",
    "> Implement the simple neural network from above as well as the squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def squared_error(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute the squared error of a predicted value, given the actual target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : (N, K) ndarrary\n",
    "        The value(s) predicted by the model.\n",
    "    truth : (N, K) ndarray\n",
    "        The actual target(s) from the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    error : (N, K) ndarray\n",
    "        The squared error(s) for the prediction(s).\n",
    "    \"\"\"\n",
    "    #[[(t11 - p11)**2, (t12 - p12)**2],\n",
    "    # [(t21 - p21)**2, (t22 - p22)**2]]\n",
    "    error = np.subtract(truth, prediction)\n",
    "    error = np.square(error)\n",
    "    return error\n",
    "\n",
    "\n",
    "def my_first_network(x, w, b):\n",
    "    \"\"\"\n",
    "    Predict a value for some input with a simple neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        The input samples to predict a value for.\n",
    "    w : (K, D) ndarray\n",
    "        The input parameters for the neural network.\n",
    "    b : (K, ) ndarray\n",
    "        The bias parameter for the neural network.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    prediction : (N, K) ndarray\n",
    "        The predicted value for the input from the network.\n",
    "    \"\"\"\n",
    "    # w × x + b or x × w**T + b\n",
    "    prediction = x.dot(w.T)\n",
    "    prediction = np.add(prediction, b)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x, y = plane_data(d=1, seed=1856)\n",
    "w, b = np.random.randn(y.shape[1], x.shape[1]), np.random.randn(y.shape[1])\n",
    "\n",
    "# print mse and visualise model\n",
    "pred = my_first_network(x, w, b)\n",
    "print(np.mean(squared_error(pred, y), axis=0).item())\n",
    "show_1d_model(x, y, w, b, lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 2: Analytical Solution (3 Points)</h3>\n",
    "\n",
    "Minimising the negative log-likelihood comes down to solving linear regression with the *least-squares* method, i.e. by minimising the sum of squared residuals. For the least-squares method in linear regression, the optimal parameters can be derived analytically. The optimal solution is given by:\n",
    "\n",
    "$$\\theta^* = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{Y}.$$\n",
    "\n",
    "Does this model incorporate a bias parameter? Where can the bias parameter be found?\n",
    "\n",
    "> Implement a function that computes this analytical solution for given inputs and outputs. The function should allow solutions both with and without bias parameter.\n",
    "\n",
    "**Hint:** to get help for commands, execute a cell with following code (for help on `np.linalg.inv`):\n",
    "```python\n",
    "?np.linalg.inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def analytical_solution(x, y, bias=True):\n",
    "    \"\"\"\n",
    "    Get the optimal parameters for linear regression,\n",
    "    given input data and target values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        Input data to the network.\n",
    "    y : (N, K) ndarray\n",
    "        Target values.\n",
    "    bias : bool, optional\n",
    "        Whether or not the network should have a bias term.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : (K, D) ndarray\n",
    "        The optimal input parameters for the network.\n",
    "    b : (K, ) ndarray\n",
    "        The optimal bias parameters for the network\n",
    "        or zero if `bias=False`.\n",
    "    \"\"\"\n",
    "    dim_N = x.shape[0]\n",
    "    dim_D = x.shape[1]\n",
    "    dim_K = y.shape[1]\n",
    "    assert dim_N == y.shape[0], \\\n",
    "        \"N dimension of input x and y differs: x\" + \\\n",
    "        str(x.shape) + \" y\" + str(y.shape)\n",
    "    #theta = ((x.T.dot(x))**-1).dot(x.T).dot(y) \n",
    "    theta = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)# D×K\n",
    "    if bias:\n",
    "        ############\n",
    "        #b = np.dot(x, theta)# N×D.D×K\n",
    "        #b = np.subtract(y, b)\n",
    "        #print(\"dim before np.mean: \" + str(b.shape))\n",
    "        #b = np.mean(a=b, axis=0, keepdims=True)\n",
    "        #print(\"dim after np.mean:  \" + str(b.shape))\n",
    "        ############\n",
    "        # b = b.T; cause it should be 1×K (row vector) (as it stands in moodle)\n",
    "        # but cause this example is with K := 1, I can't say.\n",
    "        b = np.mean(a=np.subtract(y, np.dot(x, theta)), axis=0, keepdims=True).T\n",
    "    else:\n",
    "        b = np.array([[0] * dim_K]) # right shape 1×K\n",
    "    w = theta.T\n",
    "    \n",
    "    assert w.shape[0] == dim_K and w.shape[1] == dim_D, \\\n",
    "        \"Missmatch of dimensions for W\" + str(w.shape) + \\\n",
    "        \" should be (\" + str(dim_K) + \" ,\" + str(dim_D) + \")\"\n",
    "    assert b.shape[1] == dim_K and b.shape[0] == 1, \\\n",
    "        \"Missmatch of dimension for B\" + str(b.shape) + \\\n",
    "        \" should be (\" + str(dim_K) + \", 1)\"\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x, y = plane_data(d=1, seed=1856)\n",
    "w, b = analytical_solution(x, y, bias=True)\n",
    "pred = my_first_network(x, w, b)\n",
    "print(np.mean(squared_error(pred, y), axis=0).item())\n",
    "show_1d_model(x, y, w, b, lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 3: Importance of Bias (1 Point)</h3>\n",
    "\n",
    "Whether `bias=True` or `bias=False` in the fragment above, there seems to be little difference. Why is it important to have a bias parameter? \n",
    "\n",
    "> Configure the arguments for generating data below to illustrate the problem of not having a bias parameter.\n",
    "\n",
    "**Note:** The plots below are [contour plots](https://en.wikipedia.org/wiki/Contour_line). The colours encode the value of the function in each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Bias corresponds to Offset, that is y = offset + ...\n",
    "# Without bias, we can't get an output unequal zero for an input of zero.\n",
    "# set n to 1000 to see the difference better (more dots)\n",
    "# set offset to a negative value like -1\n",
    "# we can see on the left nearly matching colors for the dots and the lines,\n",
    "# on the right the do not match by color.\n",
    "x, y = plane_data(n=1000, bound=3., noise=.1, offset=-1., seed=1856)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))  # you can change figure dimensions if necessary\n",
    "# visualise model with bias\n",
    "w, b = analytical_solution(x, y, bias=True)\n",
    "pred = my_first_network(x, w, b)\n",
    "mse = np.mean(squared_error(pred, y), axis=0)\n",
    "plt.subplot(121)\n",
    "plt.title(f\"mse: {mse.item():.5f}\")\n",
    "show_2d_model(x, y, w, b, lambda x: x)\n",
    "# visualise model without bias\n",
    "w, b = analytical_solution(x, y, bias=False)\n",
    "pred = my_first_network(x, w, b)\n",
    "mse = np.mean(squared_error(pred, y), axis=0)\n",
    "plt.subplot(122)\n",
    "plt.title(f\"mse: {mse.item():.5f}\")\n",
    "show_2d_model(x, y, w, b, lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0084bb\">Logistic Regression</h2>\n",
    "\n",
    "The values to predict are not always continuous. A very common task in machine learning is to answer yes-no questions, e.g. Is this an image of a cat? These problems are known as *binary classification* tasks. This can be encoded by labelling positive examples with a 1 and negative examples with a 0. As a result, the target values are Bernoulli distributed, rather than Gaussian. This also means that the relationship is no longer linear and linear regression makes little sense. This is where *Logistic Regression* comes in play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6 style=\"color:#0084bb\">Model</h6>\n",
    "\n",
    "Mathematically, we assume that the relation between inputs and outputs can be modelled as follows:\n",
    "\n",
    "$$\\boldsymbol{y} = \\begin{cases}\n",
    "  1 & f(\\boldsymbol{x}) > 0.5 \\\\\n",
    "  0 & f(\\boldsymbol{x}) \\leq 0.5\n",
    "\\end{cases}.$$\n",
    "\n",
    "Since the relation is non-linear, we add an additional non-linearity, $\\sigma : \\mathbb{R} \\to \\mathbb{R}$, to our simple network so that\n",
    "\n",
    "$$g(\\boldsymbol{x} \\mathbin{;} \\theta) = \\sigma\\left(\\boldsymbol{w} \\cdot \\boldsymbol{x} + \\boldsymbol{b}\\right),$$\n",
    "\n",
    "where the parameters are again $\\theta = \\{\\boldsymbol{w}, \\boldsymbol{b}\\}$. The non-linear function maps real values to values between zero and one. This allows the outputs of the network to be interpreted as the probability for its input to be classified as positive. For logistic regression, this non-linearity is the logistic sigmoid, which is defined as \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic_sigmoid(s):\n",
    "    \"\"\"\n",
    "    Compute the logistic sigmoid function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : ndarray\n",
    "        The logits to apply the logistic sigmoid function on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : ndarray\n",
    "        The probabilitie(s) for the given logit(s).\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 4: Logistic Likelihood Maximisation (3 Points)</h3>\n",
    "\n",
    "\n",
    "Just as for linear regression, maximum likelihood estimation can be used to find parameters for a model that solves the binary classification problem. Is there an error function for logistic regression, like we have the squared error for linear regression? Time to try out some [$\\LaTeX$](https://www.overleaf.com/learn)!\n",
    "\n",
    "> Derive the optimisation objective for maximum likelihood in a similar way the squared error was derived for linear regression. Implement the resulting *logistic error* in a python function.\n",
    "\n",
    "\n",
    "To derive the maximum likelihood estimator, we need the conditional probability of the label:\n",
    "> For my understanding, we use y (that has only values of {0, 1}) as k. Because we have for the function we need to implement (and I first implemented it and thought on the math behind on a little note will doing it) only the values of y and g(x; theta) are given, such that for me only this makes sense - but I think also, I'm terrible wrong.\n",
    "\n",
    "$$\\begin{align}\n",
    "  p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) & = g(\\boldsymbol{x^n} \\mathbin{;} \\theta)^{y^n} \\cdot (1 - g(\\boldsymbol{x^n} \\mathbin{;} \\theta))^{1 - y^n}\n",
    "\\\\\n",
    "  & = \\sigma\\left(\\boldsymbol{w} \\cdot \\boldsymbol{x^n} + \\boldsymbol{b}\\right)^{y^n} \\cdot (1 - \\sigma\\left(\\boldsymbol{w} \\cdot \\boldsymbol{x^n} + \\boldsymbol{b}\\right))^{1-y^n}\n",
    "\\end{align}$$\n",
    "\n",
    "The negative log-likelihood is then given by:\n",
    "$$\\begin{align}\n",
    "  -\\mathcal{l}(\\theta \\mathbin{;} \\boldsymbol{X}, \\boldsymbol{Y})\n",
    "  & = -\\sum_{n = 1}^{N} \\ln p(\\boldsymbol{y}^n \\mid \\boldsymbol{x}^n \\mathbin{;} \\theta) \\\\\n",
    "  & = -\\sum_{n = 1}^{N} \\ln \\sigma\\left(\\boldsymbol{w} \\cdot \\boldsymbol{x^n} + \\boldsymbol{b}\\right)^{y^n} \\cdot (1 - \\sigma\\left(\\boldsymbol{w} \\cdot \\boldsymbol{x^n} + \\boldsymbol{b}\\right))^{1-y^n}\n",
    "\\end{align}$$\n",
    "\n",
    "**Hint**: the probability mass function of the Bernoulli distribution, $p_\\mathrm{B}(k \\mathbin{;} p) = \\begin{cases} 1 - p & k = 0 \\\\ p & k = 1 \\end{cases}$ can be written as $p_\\mathrm{B}(k \\mathbin{;} p) = p^k (1 - p)^{1 - k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color:#0084bb\">Some Notes on LaTeX in Jupyter Notebooks</h5>\n",
    "\n",
    "$\\LaTeX$ is an advanced typesetting system that can be used for all sorts of documents. One of the key features of $\\LaTeX$ is the ability to insert mathematical formulas in the text. Jupyter notebooks allow to display mathematical symbols (and some other things) through $\\LaTeX$ syntax. Technically the conversion from $\\LaTeX$ code to HTML is done by [MathJax](https://www.mathjax.org/), so if you cannot see the nice formulas I put in this notebook, you might need to troubleshoot MathJax.\n",
    "\n",
    "To give you an idea of the possibilities of $\\LaTeX$:\n",
    "\n",
    "| code                   | rendered             | code (big)               | rendered (big)         |\n",
    "|:---------------------- | --------------------:|:------------------------ | ----------------------:|\n",
    "| `$\\LaTeX$`             | $\\LaTeX$             |                          |                        |\n",
    "| `$\\ln \\boldsymbol{x}$` | $\\ln \\boldsymbol{x}$ | `$$\\ln \\boldsymbol{x}$$` | $$\\ln \\boldsymbol{x}$$ |\n",
    "| `$\\frac{a}{b}$`        | $\\frac{a}{b}$        | `$$\\frac{a}{b}$$`        | $$\\frac{a}{b}$$        |\n",
    "| `$\\sum_{i=1}^I i^2$`   | $\\sum_{i=1}^I i^2$   | `$$\\sum_{i=1}^I i^2$$`   | $$\\sum_{i=1}^I i^2$$   |\n",
    "\n",
    "If you have troubles to find the right symbols, you can use [detexify](http://detexify.kirelabs.org/classify.html) or directly use a more graphical $\\LaTeX$ equation editor, e.g. [this one from codecogs](https://www.codecogs.com/latex/eqneditor.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic_error(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute the logistic error of a predicted value, given the actual target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : (N, K) ndarrary\n",
    "        The value(s) predicted by the model.\n",
    "    truth : (N, K) ndarray\n",
    "        The actual target(s) from the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    error : (N, K) ndarray\n",
    "        The logistic error(s) for the prediction(s).\n",
    "    \"\"\"\n",
    "    # truth is y: are values of {0, 1}\n",
    "    # prediction is allready sigmoided\n",
    "    #\n",
    "    # if truth: prediction\n",
    "    # else:     1 - prediction\n",
    "    dim_N = prediction.shape[0]\n",
    "    dim_K = prediction.shape[1]\n",
    "\n",
    "    assert truth.shape[0] == dim_N, str(truth.shape[0]) + \" != \" + str(dim_N)\n",
    "    assert truth.shape[1] == dim_K, str(truth.shape[1]) + \" != \" + str(dim_K)\n",
    "    \n",
    "    t = np.power(prediction, truth)\n",
    "    f = np.power(np.subtract(1, prediction), np.subtract(1, truth))\n",
    "    error = np.multiply(t, f)\n",
    "    \n",
    "    assert error.shape[0] == dim_N, str(error.shape[0]) + \" != \" + str(dim_N)\n",
    "    assert error.shape[1] == dim_K, str(error.shape[1]) + \" != \" + str(dim_K)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# fixed version as posted in moodle\n",
    "x, y = blob_data(d=1, seed=1856)\n",
    "y = y.reshape(-1,1)\n",
    "w, b = np.random.randn(1, x.shape[1]), np.random.randn(1)\n",
    "\n",
    "# print mle and visualise model\n",
    "logits = my_first_network(x, w, b)\n",
    "pred = logistic_sigmoid(logits)\n",
    "print(np.mean(logistic_error(pred, y), axis=0))\n",
    "show_1d_model(x, y, w, b, logistic_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 5: Gradient Descent (3 Points)</h3>\n",
    "\n",
    "Since there is no closed form solution for logistic regression, other methods are necessary to find the optimal parameters. Gradient descent is one of the simplest methods to get to a good solution. By pushing the parameters in the opposite direction of the gradient, it is possible to find local minima relatively fast.\n",
    "\n",
    "Concretely, the gradient descent algorithm computes the gradients for some inputs and moves a certain amount in the direction of that gradient. This process is repeated a number of times until convergence. The amount of gradient to go down is known as the *learning rate*, and a single step of computing gradients and updating parameters is commonly called an *epoch*.\n",
    "\n",
    "> Implement the gradient descent step, i.e. one epoch, for logistic regression. Make sure not to forget any parameters in the gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(step_func, x, y, bias=True, epochs=10, lr=1.):\n",
    "    \"\"\"\n",
    "    Use gradient descent to compute the weights of a network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    step_func : callable\n",
    "        Function that implements the gradient descent step.\n",
    "        Signature should correspond to `err = step_func(x, y, w, b, lr)`.\n",
    "    x : ndarray\n",
    "        Inputs to the network.\n",
    "    y : ndarray\n",
    "        Targets for the network.\n",
    "    bias : bool, optional\n",
    "        Whether or not the network should have a bias term.\n",
    "    epochs : int, optional\n",
    "        Number of steps to descent.\n",
    "    lr : float, optional\n",
    "        Scaling factor for gradient steps.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray\n",
    "        The optimal input parameters for the network.\n",
    "    b : ndarray, optional\n",
    "        The optimal bias parameters for the network.\n",
    "        Only returned if `bias=True`.\n",
    "    \"\"\"\n",
    "    w = np.zeros((y.shape[1], x.shape[1]))\n",
    "    b = np.zeros(y.shape[1])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        err = step_func(x, y, w, b, lr=lr)\n",
    "        \n",
    "        if not bias:\n",
    "            # undo bias updates\n",
    "            b[:] = 0\n",
    "            \n",
    "        # print mean error at most 10 times during learning\n",
    "        if epochs < 10 or i % (epochs // 10) == 0:\n",
    "            print(np.mean(err, axis=0).item())\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "\n",
    "def logistic_gradient_descent_step(x, y, w, b, lr=1.):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the logistic error w.r.t. the parameters\n",
    "    and apply update to the parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        Inputs to the network.\n",
    "    y : (N, K) ndarray\n",
    "        Targets for the network.\n",
    "    w : (K, D) ndarray\n",
    "        Input weights of the network.\n",
    "    b : (K, ) ndarray\n",
    "        Bias weights of the network.\n",
    "    lr : float, optional\n",
    "        Scaling factor for gradient steps.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    err : (N, K) ndarray\n",
    "        The error of the prediction from the network \n",
    "        for the given input data before the update.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Updates the network parameters in-place.\n",
    "    \"\"\"\n",
    "    logits = my_first_network(x, w, b) # N×K\n",
    "    pred = logistic_sigmoid(logits)    # N×K\n",
    "    err = logistic_error(pred, y)      # N×K\n",
    "    \n",
    "    ##\n",
    "    # w\n",
    "    dim_N = x.shape[0]\n",
    "    grad = (np.dot(x.T, np.subtract(y, pred)) ) * lr # D×K\n",
    "    w += grad.T # K×D, why plus not minus? but the result looks fine\n",
    "    \n",
    "    # b - nothing found in our scripts, but this seams to work, multiplicate with lr is to small\n",
    "    b[:] += np.mean(a=np.subtract(y, pred), axis=0, keepdims=False) #* lr # 1×K\n",
    "    ##\n",
    "    \n",
    "    return err\n",
    "\n",
    "\n",
    "def logistic_gradient_descent(x, y, bias=True, epochs=10, lr=1e-3):\n",
    "    return gradient_descent(logistic_gradient_descent_step, x, y[:, None], bias, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x, y = blob_data(d=1, seed=1856)\n",
    "w, b = logistic_gradient_descent(x, y, epochs=100, bias=True)\n",
    "show_1d_model(x, y, w, b, logistic_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0084bb\">Softmax Regression</h2>\n",
    "\n",
    "Softmax regression is essentially a generalisation of binary logistic regression that allows to solve *multi-class problems*, i.e. questions with more than two possible (exclusive) answers. Instead of predicting a single probability, the goal in softmax regression is to predict a probability for each of the possible classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 6: One-hot Encoding (1 Point)</h3>\n",
    "\n",
    "Since the labels are mostly integer values, some encoding is necessary to have the targets reflect probabilities. E.g. if there are three classes, the labels would be `0`, `1`, `2`. The most common way to encode these labels as probability vectors is to use a one-hot encoding. A one-hot code uses as much bits as there are labels. The code is zero everywhere, except at the index that corresponds to the label, where it is one. E.g. in the case of the three zero-indexed labels, the encoding for the label `1` would be `[0, 1, 0]`\n",
    "\n",
    "> Implement a function to compute one-hot encodings from integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y, k=None):\n",
    "    \"\"\"\n",
    "    Compute a one-hot encoding from a vector of integer labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : (N, ) ndarray\n",
    "        The zero-indexed integer labels to encode.\n",
    "    k : int, optional\n",
    "        The number of distinct labels in `y`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    one_hot : (N, k) ndarray\n",
    "        The one-hot encoding of the labels.\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    if k is None:\n",
    "        k = len(np.unique(y))\n",
    "    \n",
    "    #mapping = [-1] * k\n",
    "    #for i in range(n):\n",
    "    #    if y[i] in mapping:\n",
    "    #        pos = mapping.index(y[i])\n",
    "    #    else:\n",
    "    #        pos = mapping.index(-1)\n",
    "    #        mapping[pos] = y[i]\n",
    "    #    row = [0] * k\n",
    "    #    row[pos] = 1\n",
    "    #    if i == 0:\n",
    "    #        one_hot = [row]\n",
    "    #    else:\n",
    "    #        one_hot = np.vstack([one_hot, row])\n",
    "    for i in range(n):\n",
    "        row = [0] * k\n",
    "        row[y[i]] = 1\n",
    "        if i == 0:\n",
    "            one_hot = [row]\n",
    "        else:\n",
    "            one_hot = np.vstack([one_hot, row])\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(3, size=5)\n",
    "print(\"original y:\")\n",
    "print(y)\n",
    "print(\"result:\")\n",
    "print(to_one_hot(y, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#0084bb\">Exercise 7: Softmax Gradients (2 Bonus Points)</h3>\n",
    "\n",
    "Time to generalise logistic gradient descent to handle multiple labels! Either you remember these functions from the lecture or you can freshen up your memory in the script. \n",
    "\n",
    "> Implement the functions necessary for training a softmax classifier with gradient descent. Make sure not to forget any parameters in the gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    \"\"\"\n",
    "    Compute the softmax function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : (N, K) ndarray\n",
    "        The logits to apply the softmax function on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : (N, K) ndarray\n",
    "        The probabilitie(s) for the given logit(s).\n",
    "    \"\"\"\n",
    "    dim_N = s.shape[0]\n",
    "    dim_K = s.shape[1]\n",
    "    s = np.exp(s)\n",
    "    div = np.sum(s, axis=1, keepdims=True)\n",
    "    assert div.shape[0] == dim_N\n",
    "    assert div.shape[1] == 1\n",
    "    s = np.divide(s, div)\n",
    "    \n",
    "    np.testing.assert_allclose(np.sum(s, axis=1), 1)\n",
    "    assert s.shape[0] == dim_N\n",
    "    assert s.shape[1] == dim_K\n",
    "    return s\n",
    "\n",
    "\n",
    "def cross_entropy(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute the logistic error of a predicted value, given the actual target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : (N, K) ndarrary\n",
    "        The value(s) predicted by the model.\n",
    "    truth : (N, K) ndarray\n",
    "        The actual target(s) from the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    error : (N, ) ndarray\n",
    "        The logistic error(s) for the prediction(s).\n",
    "    \"\"\"\n",
    "    # Hint: check the sign!\n",
    "    raise NotImplementedError(\"TODO: implement cross_entropy function!\")\n",
    "\n",
    "\n",
    "def softmax_gradient_descent_step(x, y, w, b, lr=1.):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross-entropy w.r.t. the parameters\n",
    "    and apply update to the parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, D) ndarray\n",
    "        Inputs to the network.\n",
    "    y : (N, K) ndarray\n",
    "        Targets for the network.\n",
    "    w : (K, D) ndarray\n",
    "        Input weights of the network.\n",
    "    b : (K, ) ndarray\n",
    "        Bias weights of the network.\n",
    "    lr : float, optional\n",
    "        Scaling factor for gradient steps.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    err : (N, ) ndarray\n",
    "        The error of the prediction from the network \n",
    "        for the given input data before the update.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Updates the network parameters in-place.\n",
    "    \"\"\"\n",
    "    logits = my_first_network(x, w, b)\n",
    "    pred = softmax(logits)\n",
    "    err = cross_entropy(pred, y)\n",
    "    \n",
    "    raise NotImplementedError(\"TODO: implement softmax_gradient_descent_step function!\")\n",
    "    \n",
    "    return err\n",
    "\n",
    "def softmax_gradient_descent(x, y, bias=True, epochs=10, lr=1e-3):\n",
    "    return gradient_descent(softmax_gradient_descent_step, x, y, bias, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2  # feel free to play around!\n",
    "x, y = blob_data(k=num_classes, seed=1856)\n",
    "y_hot = to_one_hot(y, k=num_classes)\n",
    "w, b = softmax_gradient_descent(x, y_hot, epochs=100, bias=True)\n",
    "show_2d_model(x, y, w, b, lambda x: np.sum(softmax(x) * np.arange(num_classes), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
